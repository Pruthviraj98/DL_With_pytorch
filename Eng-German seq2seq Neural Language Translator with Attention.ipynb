{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM\n",
    "\n",
    "The low level language translation can ba done just by looking into the dictionary of the respective languages or searching the meaning of words in the required language. But, it is not necessary that the words in one language mean the same in the other language as well when translated in the given context. Hence, in this work the language translation along with the context understanding is done using the seq2seq model of RNN to enhancify the Neural Language translation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # for softmax operation\n",
    "\n",
    "from torchtext.datasets import Multi30k#dataset\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to ensure results are reproducable set these\n",
    "SEED=2031\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en=spacy.load(\"en_core_web_sm\")\n",
    "spacy_ger=spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to tokenize\n",
    "def tokenize_english(sentance):\n",
    "    return([tok.text for tok in spacy_en.tokenizer(sentance)]) \n",
    "def tokenize_german(sentance):\n",
    "    return([tok.text for tok in spacy_ger.tokenizer(sentance)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of the torchtext Field module\n",
    "\n",
    "the Field module in the torchtext provides a handy module for how the data is to be processed here, Field modules is used for tokenization along with converting all the  tokens into the lower case with the sequence to sequence model, the sentances are padded with the start of string and the end of string token in the beginning and the end of sentance respectively.\n",
    "\n",
    "\n",
    "So, seq to seq models start generating tokens as soon as it sees the start of string token and continues untill it sees the end of string token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=Field(tokenize=tokenize_german, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "target=Field(tokenize=tokenize_english, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data=Multi30k.splits(exts=('.de', '.en'), fields=(source, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus building\n",
    "\n",
    "Building the vocabulary for tokens in each language so that each token within the language has an index and the index used for one hot encoded representation internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.build_vocab(train_data, min_freq=2)\n",
    "target.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Iterators\n",
    "\n",
    "To create the batches out of the dataset, iterators are created. Sort them, pad them and move them to the appropriate device. This can be done by bucket interators. These iterators return batches of data which will have src attribute and trg attribute. Further, all the attributes would be converted into index form. \n",
    "\n",
    "###### Advantage of using Bucket iterator: \n",
    "\n",
    "Bucket iterators create the iterators in such a way that it requires minumum amount of padding in each batch collecting similar length sentances together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batch_size=128\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator\n",
    "train_it, valid_it, test_it=BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=Batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "Building the seq2seq model with attention\n",
    "\n",
    "### 1. Encoder\n",
    "\n",
    "Within the encoder class, GRU unit is used. A single layer of GRU is used without any drop outs. In each step of  GRU it takes previous hidden state and the current input token that is passed on to the encoder. Encoder class is the sub class of the nn module.The GRU used here is bidirectional.\n",
    "\n",
    "Here, encoder hidden dim and the decoder hid dim differs.\n",
    "\n",
    "##### The encoders final hidden state is the concatenation of the forward and the backward hidden states, However, the decoder only has a forward unit only (one direction). So, here, the last two states are concatenated and pass it through the dense layer applying the tanh operation to restructure our output in the required dimension dimension for the decoder.\n",
    "\n",
    "Unlike the previous architecture,all the outputs of encoders (all the hidden states) are returned that were generated while reading the inputs. The outputs are required by the attention to create the associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, inp_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout): \n",
    "        super().__init__()\n",
    "        self.input_dim=inp_dim #vocabulary size of the source language (german) \n",
    "        self.emb_dim=emb_dim #output dimension of embedding layer\n",
    "        self.encoder_hid_dim= enc_hid_dim # GRUs output dimension\n",
    "        self.decoder_hid_dim= dec_hid_dim\n",
    "        self.embedding=nn.Embedding(inp_dim, emb_dim)\n",
    "        self.rnn=nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)#Bidirectional GRU\n",
    "        self.dropout=nn.Dropout(dropout) #using this for embedding layer\n",
    "        self.dense=nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
    "    \n",
    "    def forward(self, source): \n",
    "        embedded=self.dropout(self.embedding(source))#shape of the source= [sentance len, batch_size]\n",
    "        outputs, hidden=self.rnn(embedded)#shape of embedded is= [sentance len, batch size, emb_dim]\n",
    "        #shape of output=[sentance len, batch_size, hid_dim*n directions] here no. of directions is one as we are not using the bidirectional GRU\n",
    "        #shape of hidden=[n layers, ndirections, batch_size*hidden_dim ]\n",
    "        hidden=torch.tanh(self.dense(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        \n",
    "        return outputs, hidden  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dm, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.encoder_hid_dim= enc_hid_dim # GRUs output dimension\n",
    "        self.decoder_hid_dim= dec_hid_dim\n",
    "        self.attention=nn.Linear((enc_hid_dim*2)+dec_hid_dim, dec_hid_dim)\n",
    "        self.vec=nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        #hidden size=[batch_size, dec_hid_dim]\n",
    "        #encoder_outputs size=[src set len, batch_size, encoder_hid_dim*2]\n",
    "        batch_size=encoder_outputs.shape[1] #cols\n",
    "        src_len=encoder_outputs.shape[0] #rows\n",
    "        \n",
    "        #now repeat the hidden state till the source length number of times\n",
    "        hidden=hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs=encoder_outputs.permute(1, 0, 2)\n",
    "        #hidden_shape=[batch_size, src_sent_len, dec_hid_size]\n",
    "        #encoder_outputs_shape=[batch_size, src_sent_len, enc_hid_dim*2]\n",
    "        #concatenate them and pass them to dense layer\n",
    "        association=torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        #association_shape=[batch_size, src_sent_len, dec_hid_size]#cuz of dense\n",
    "        \n",
    "        #reshape the association tensor\n",
    "        association=association.permute(0, 2, 1)\n",
    "        #association shape=[batch_size, dec_hid_dim, src_sent_len]\n",
    "        #reshape the vec tensor\n",
    "        vec=self.vec.repeat(batch_size, 1).unsqueeze(1)\n",
    "        #vec shape= [batch_size, 1, dec_hid_dim]\n",
    "        #obtain the product of the vec and association and squeeze the extra dimension\n",
    "        attention=torch.bmm(vec, association).squeeze(1)\n",
    "        #attention shape=[batch_size, src_sent_len]\n",
    "        return (F.softmax(attention, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decoder\n",
    "\n",
    "\n",
    "Different step from conventional seq2seq model. Here, the initial hidden state is the output from the encoder (context vector). The decoder has to decode the current token based on the memory of the context vector that it has seen in the first place. This affects decoders capability to generate the token after n time steps.\n",
    "\n",
    "##### The intuition used here is:\n",
    "    The context vector is passed along with the previous hidden state and their current target token in each time step. Comparing to the conventional seq2seq model, which generates output based on the previous hidden state and the current token. However, here the output is produced based on the current input token, context vector and previous hidden state. By passing the same context vector over and over again in each time step. Thus the  GRU's input dimension in decoder looks like (emb_dim+hid_dim, hid_dim)\n",
    "    \n",
    "    \n",
    "Linear(dense) layer being the top layer of the decoder\n",
    "\n",
    "\n",
    "###### In case of Forward pass:\n",
    "\n",
    "The concatenation of current input token and the context vector before feeding to the GRU is done.\n",
    "Also, the current hidden state, current token, context vector are concatenated before passing to the dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.emb_dim=emb_dim #output dimension of embedding layer\n",
    "        self.encoder_hid_dim= enc_hid_dim # GRUs output dimension\n",
    "        self.out_dim=out_dim #target corpus dim\n",
    "        self.attention=attention\n",
    "        self.embedding=nn.Embedding(out_dim, emb_dim)\n",
    "        self.rnn=nn.GRU((enc_hid_dim*2)+emb_dim, dec_hid_dim)\n",
    "        self.dense=nn.Linear((enc_hid_dim*2)+dec_hid_dim+emb_dim, out_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        #input size= [batch_size]\n",
    "        #(context) hidden size= [batch_size, dec_hid_dim] \n",
    "        #encoder outputs size= [source_sent_len, batch_size, enc_hid_dim*2]\n",
    "        \n",
    "        input=input.unsqueeze(0)\n",
    "        #input size=[1, batch_size]\n",
    "        embedded=self.dropout(self.embedding(input)) #out shape = [1, batch_size, emb_dim]\n",
    "        a=self.attention(hidden, encoder_outputs)\n",
    "        #a shape=[batch_size, src_len]\n",
    "        a=a.unsqueeze(1)\n",
    "        #a shape=[batch_size, 1, src_len]\n",
    "        encoder_outputs=encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs shape=[batch_size, source_sent_len, enc_hid_dim*2]\n",
    "        \n",
    "        #performing batch matrix mult\n",
    "        weighted=torch.bmm(a, encoder_outputs)\n",
    "        #weighted_shape=[batch_size, 1, enc_hid_dim*2]\n",
    "        \n",
    "        #concatenate weighted with embedded input representation to be fed into the GRU along with prev hidd state\n",
    "        weighted=weighted.permute(1, 0, 2)\n",
    "        #weighted_shape=[1, batch_size, enc_hid_dim*2]\n",
    "        \n",
    "        rnn_input=torch.cat((embedded, weighted), dim=2)\n",
    "        #rnn_input_shape=[1, batch_size, (end_hid_dim)*2+emb_dim]\n",
    "        \n",
    "        output, hidden=self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        #output=[sent_len, batch_size, dec_hid_dim*n_dir]\n",
    "        #hidden=[n_layers*n_dir, batch_size, dec_hid_dim]\n",
    "        #output=[1, batch_size, dec_hid_dim]\n",
    "        #hidden=[1, batch_size, dec_hid_dim]\n",
    "        #weighted_shape=[1, batch_size, enc_hid_dim*2]\n",
    "\n",
    "        #removing the extra unwanted dimension\n",
    "        embedded=embedded.squeeze(0)\n",
    "        output=output.squeeze(0)\n",
    "        weighted=weighted.squeeze(0)\n",
    "        \n",
    "        output=self.dense(torch.cat((output, weighted, embedded), dim=1))\n",
    "        #output=[batch_size, emb_dim]\n",
    "        return output, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq class which binds the encoder and decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.device=device\n",
    "        \n",
    "    def forward(self, source, target, tfr=0.5): #tfr= teacherforcing ratio is the probability value that determines weather actual ground truth token from the target has to be taken or the prediction from the decoder to be taken while predicting the next target token\n",
    "    #if the tfr is 0.25, in 25% of cases the ground truth token is used and rest of the 75% is taken as the prediction from the decoder\n",
    "    #source shape=[sen_len, batch_size]\n",
    "    #target shape=[sen_len, batch_size]\n",
    "        batch_size=target.shape[1]\n",
    "        max_len=target.shape[0]\n",
    "        target_vec_size=self.decoder.out_dim\n",
    "        #initialize the tensor that holds output from decoder (i.e. the tensor initialized to zeros)\n",
    "        outputs=torch.zeros(max_len, batch_size, target_vec_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden =self.encoder(source)#forms the initial hidden state for the decoder\n",
    "        input=target[0, :]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden=self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t]=output\n",
    "            tforce=random.random()<tfr\n",
    "            top1=output.max(1)[1]\n",
    "            input=(target[t] if tforce else top1)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the train function, loss function and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dim=len(source.vocab)\n",
    "out_dim=len(target.vocab)\n",
    "encoder_emb_dim=256\n",
    "decoder_emb_dim=256\n",
    "enc_hid_dim=512\n",
    "dec_hid_dim=512\n",
    "encoder_dropout=0.5\n",
    "decoder_dropout=0.5\n",
    "\n",
    "attention=Attention(enc_hid_dim, dec_hid_dim)\n",
    "encoder=Encoder(inp_dim, encoder_emb_dim, enc_hid_dim, dec_hid_dim, encoder_dropout)\n",
    "decoder=Decoder(out_dim, decoder_emb_dim, enc_hid_dim, dec_hid_dim, decoder_dropout, attention)\n",
    "model=seq2seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attention): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (dense): Linear(in_features=1792, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the loss for the padded tokens since is  not calculated since they are not the part of the actual source and target sequences. Thus, here the padded index is ignored while calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters())\n",
    "\n",
    "padded_index=target.vocab.stoi['<pad>']\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=padded_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train, validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip): #clip= to prevent gradient explosion\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    train_loss=[]\n",
    "    for i, batch in enumerate(iterator):\n",
    "        source=batch.src\n",
    "        target=batch.trg #size= [sentlen, batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        output=model(source, target)#size= [sent_len, batch_size, output_dim]\n",
    "    #flatten the output by using the view method and  ignore the sos padding of source and the target sentances before calculating the loss\n",
    "        loss=criterion(output[1:].view(-1, output.shape[2]), target[1:].view(-1)) #so the first column of the output and the target are removed and passed into the criterion\n",
    "        #while passing to loss fn= output[(sent_len-1)*batch_size, output_size] \n",
    "        #target=[(sent_len-1)*batch_size, output_size]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        train_loss.append(epoch_loss)\n",
    "        \n",
    "    return(epoch_loss/len(iterator), train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss=0\n",
    "    val_loss=[]\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            source=batch.src\n",
    "            target=batch.trg\n",
    "            optimizer.zero_grad()\n",
    "            output=model(source, target, 0)#clip is zero and no train\n",
    "            loss=criterion(output[1:].view(-1, output.shape[2]), target[1:].view(-1))\n",
    "            epoch_loss+=loss.item()\n",
    "            val_loss.append(epoch_loss)\n",
    "        \n",
    "    return(epoch_loss/len(iterator), val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Set the path and name of the model to save them after trining them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR='models'\n",
    "model_dir=os.path.join(DIR, 'seq2seq_model_atten.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test model along with analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "clip=10\n",
    "\n",
    "best_loss=float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{DIR}'):\n",
    "    os.makedirs(f'{DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number=  0\n",
      "Train loss=  4.461344998313467\n",
      "Validation loss= 4.068058580160141\n",
      "epoch number=  1\n",
      "Train loss=  3.4776270515593137\n",
      "Validation loss= 3.7529342472553253\n",
      "epoch number=  2\n",
      "Train loss=  3.0673553365967874\n",
      "Validation loss= 3.626277804374695\n",
      "epoch number=  3\n",
      "Train loss=  2.7754533637462733\n",
      "Validation loss= 3.5566119253635406\n",
      "epoch number=  4\n",
      "Train loss=  2.560871342730417\n",
      "Validation loss= 3.5911119282245636\n",
      "epoch number=  5\n",
      "Train loss=  2.3784026359146386\n",
      "Validation loss= 3.53265181183815\n",
      "epoch number=  6\n",
      "Train loss=  2.2311025936698075\n",
      "Validation loss= 3.590732663869858\n",
      "epoch number=  7\n",
      "Train loss=  2.1109483252537933\n",
      "Validation loss= 3.588593691587448\n",
      "epoch number=  8\n",
      "Train loss=  2.0279367715776755\n",
      "Validation loss= 3.6424492597579956\n",
      "epoch number=  9\n",
      "Train loss=  1.9450143491644165\n",
      "Validation loss= 3.567496955394745\n"
     ]
    }
   ],
   "source": [
    "train_losses=[]\n",
    "val_losses=[]\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_loss_list=train(model, train_it, optimizer, criterion, clip)\n",
    "    validation_loss, val_loss_list=evaluate(model, valid_it, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(validation_loss)\n",
    "    \n",
    "    if(validation_loss <best_loss):\n",
    "        best_loss=validation_loss\n",
    "        torch.save(model.state_dict(), model_dir)\n",
    "    \n",
    "    print(\"epoch number= \", epoch)\n",
    "    print(\"Train loss= \",train_loss)\n",
    "    print(\"Validation loss=\", validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if one wants to plot the train and validation loss graph \n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.plot(train_losses, val_losses)\n",
    "# plt.xlabel(\"loss\")\n",
    "# plt.ylabel(\"epochs\")\n",
    "# plt.title(\"Train vs Validation graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 3.520799547433853\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_dir))\n",
    "test_loss, test_losses=evaluate(model, test_it, criterion)\n",
    "print(\"loss=\", test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
