{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM\n",
    "\n",
    "The low level language translation can ba done just by looking into the dictionary of the respective languages or searching the meaning of words in the required language. But, it is not necessary that the words in one language mean the same in the other language as well when translated in the given context. Hence, in this work the language translation along with the context understanding is done using the seq2seq model of RNN to enhancify the Neural Language translation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import Multi30k#dataset\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to ensure results are reproducable set these\n",
    "SEED=2031\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en=spacy.load(\"en_core_web_sm\")\n",
    "spacy_ger=spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to tokenize\n",
    "def tokenize_english(sentance):\n",
    "    return([tok.text for tok in spacy_en.tokenizer(sentance)]) \n",
    "def tokenize_german(sentance):\n",
    "    return([tok.text for tok in spacy_ger.tokenizer(sentance)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of the torchtext Field module\n",
    "\n",
    "the Field module in the torchtext provides a handy module for how the data is to be processed here, Field modules is used for tokenization along with converting all the  tokens into the lower case with the sequence to sequence model, the sentances are padded with the start of string and the end of string token in the beginning and the end of sentance respectively.\n",
    "\n",
    "\n",
    "So, seq to seq models start generating tokens as soon as it sees the start of string token and continues untill it sees the end of string token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=Field(tokenize=tokenize_german, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "target=Field(tokenize=tokenize_english, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data=Multi30k.splits(exts=('.de', '.en'), fields=(source, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus building\n",
    "\n",
    "Building the vocabulary for tokens in each language so that each token within the language has an index and the index used for one hot encoded representation internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.build_vocab(train_data, min_freq=2)\n",
    "target.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Iterators\n",
    "\n",
    "To create the batches out of the dataset, iterators are created. Sort them, pad them and move them to the appropriate device. This can be done by bucket interators. These iterators return batches of data which will have src attribute and trg attribute. Further, all the attributes would be converted into index form. \n",
    "\n",
    "###### Advantage of using Bucket iterator: \n",
    "\n",
    "Bucket iterators create the iterators in such a way that it requires minumum amount of padding in each batch collecting similar length sentances together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batch_size=128\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator\n",
    "train_it, valid_it, test_it=BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=Batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "Building the seq2seq model\n",
    "\n",
    "### 1. Encoder\n",
    "\n",
    "Within the encoder class, GRU unit is used. A single layer of GRU is used without any drop outs. In each step of  GRU it takes previous hidden state and the current input token that is passed on to the encoder. Encoder class is the sub class of the nn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, inp_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim=inp_dim #vocabulary size of the source language (german) \n",
    "        self.emb_dim=emb_dim #output dimension of embedding layer\n",
    "        self.hidden_dim=hid_dim# GRUs output dimension\n",
    "        \n",
    "        self.embedding=nn.Embedding(inp_dim, emb_dim)\n",
    "        self.rnn=nn.GRU(emb_dim, hid_dim)\n",
    "        self.dropout=nn.Dropout(dropout) #using this for embedding layer\n",
    "        \n",
    "    def forward(self, source): #input source sentances batch, which by help of embedding layer converted into dense representations\n",
    "        #then, those dense representations passed on to GRUs.\n",
    "        #unlike LSTMs GRU doesn't return cell state\n",
    "        \n",
    "        embedded=self.dropout(self.embedding(source))#shape of the source= [sentance len, batch_size]\n",
    "        outputs, hidden=self.rnn(embedded)#shape of embedded is= [sentance len, batch size, emb_dim]\n",
    "        \n",
    "        #shape of output=[sentance len, batch_size, hid_dim*n directions] here no. of directions is one as we are not using the bidirectional GRU\n",
    "        #shape of hidden=[n layers, ndirections, batch_size*hidden_dim ]\n",
    "        \n",
    "        return hidden #the thought or the context vector        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decoder\n",
    "\n",
    "\n",
    "Different step from conventional seq2seq model. Here, the initial hidden state is the output from the encoder (context vector). The decoder has to decode the current token based on the memory of the context vector that it has seen in the first place. This affects decoders capability to generate the token after n time steps.\n",
    "\n",
    "##### The intuition used here is:\n",
    "    The context vector is passed along with the previous hidden state and their current target token in each time step. Comparing to the conventional seq2seq model, which generates output based on the previous hidden state and the current token. However, here the output is produced based on the current input token, context vector and previous hidden state. By passing the same context vector over and over again in each time step. Thus the  GRU's input dimension in decoder looks like (emb_dim+hid_dim, hid_dim)\n",
    "    \n",
    "    \n",
    "Linear(dense) layer being the top layer of the decoder\n",
    "\n",
    "\n",
    "###### In case of Forward pass:\n",
    "\n",
    "The concatenation of current input token and the context vector before feeding to the GRU is done.\n",
    "Also, the current hidden state, current token, context vector are concatenated before passing to the dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.hid_dim=hid_dim\n",
    "        self.out_dim=out_dim #target corpus dim\n",
    "        self.embedding=nn.Embedding(out_dim, emb_dim)\n",
    "        self.rnn=nn.GRU(emb_dim+hid_dim, hid_dim)#input dimension comes from the current token(emb_dim) and the context vector(hid_dim) and output is hid_dim\n",
    "        self.dense=nn.Linear(emb_dim+hid_dim*2, out_dim) #input - emb_dim(current token) and the hid_dim*2(context vector + current hidden state) is passed and output is out_dimension\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, context):\n",
    "        #input size= [batch_size]\n",
    "        #hidden size= [n_layers*ndirections, batch_size, hid_dim] = [1*1, batch_size, hid_dim]\n",
    "        #context size= [n_layers*n_directions, batch_size, hid_dim] = [1*1, batch_size, hid_dim]\n",
    "        \n",
    "        input=input.unsqueeze(0)\n",
    "        #input size=[1, batch_size]\n",
    "        embed=self.dropout(self.embedding(input)) #shape = [1, batch_size, emb_dim]\n",
    "        concatenated_embed_context=torch.cat((embed, context), dim=2)\n",
    "        #now, its size becomes [1, batch_size, emb_dim+hid_dim]\n",
    "        output, hidden=self.rnn(concatenated_embed_context, hidden)\n",
    "        #output size=[sent_len, batch_size, hid_dim*ndirection] = [1, batch_size, hid_dim]\n",
    "        #hidden size=[n_layers*n_directions, batch_size, hid_dim] = [1, batch_size, hid_dim]\n",
    "        #since, only one token at a time is predicted, the sentance length is going to be one\n",
    "        #now, concatenating current hidden state, current token, context vector and pass it to dense layer\n",
    "        output=torch.cat((embed.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1)\n",
    "        #output size= [batch_size, emb_dim+hid_dim*2] where emb_dim+hid_dim*2 comes from the current token, current hidden, context\n",
    "        prediction=self.dense(output)\n",
    "        return prediction, hidden #returned the prediction, hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq class which binds the encoder and decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.device=device\n",
    "        \n",
    "    def forward(self, source, target, tfr=0.5): #tfr= teacherforcing ratio is the probability value that determines weather actual ground truth token from the target has to be taken or the prediction from the decoder to be taken while predicting the next target token\n",
    "    #if the tfr is 0.25, in 25% of cases the ground truth token is used and rest of the 75% is taken as the prediction from the decoder\n",
    "    #source shape=[sen_len, batch_size]\n",
    "    #target shape=[sen_len, batch_size]\n",
    "        batch_size=target.shape[1]\n",
    "        max_len=target.shape[0]\n",
    "        target_vec_size=self.decoder.out_dim\n",
    "        #initialize the tensor that holds output from decoder (i.e. the tensor initialized to zeros)\n",
    "        outputs=torch.zeros(max_len, batch_size, target_vec_size).to(self.device)\n",
    "        \n",
    "        context=self.encoder(source)#forms the initial hidden state for the decoder\n",
    "        hidden=context\n",
    "        input=target[0, :]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden=self.decoder(input, hidden, context)\n",
    "            outputs[t]=output\n",
    "            tforce=random.random()<tfr\n",
    "            top1=output.max(1)[1]\n",
    "            input=(target[t] if tforce else top1)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the train function, loss function and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dim=len(source.vocab)\n",
    "out_dim=len(target.vocab)\n",
    "encoder_emb_dim=256\n",
    "decoder_emb_dim=256\n",
    "hid_dim=512\n",
    "encoder_dropout=0.5\n",
    "decoder_dropout=0.5\n",
    "\n",
    "encoder=Encoder(inp_dim, encoder_emb_dim, hid_dim, encoder_dropout)\n",
    "decoder=Decoder(out_dim, decoder_emb_dim, hid_dim, decoder_dropout)\n",
    "model=seq2seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(768, 512)\n",
       "    (dense): Linear(in_features=1280, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the loss for the padded tokens since is  not calculated since they are not the part of the actual source and target sequences. Thus, here the padded index is ignored while calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters())\n",
    "\n",
    "padded_index=target.vocab.stoi['<pad>']\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=padded_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train, validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip): #clip= to prevent gradient explosion\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    train_loss=[]\n",
    "    for i, batch in enumerate(iterator):\n",
    "        source=batch.src\n",
    "        target=batch.trg #size= [sentlen, batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        output=model(source, target)#size= [sent_len, batch_size, output_dim]\n",
    "    #flatten the output by using the view method and  ignore the sos padding of source and the target sentances before calculating the loss\n",
    "        loss=criterion(output[1:].view(-1, output.shape[2]), target[1:].view(-1)) #so the first column of the output and the target are removed and passed into the criterion\n",
    "        #while passing to loss fn= output[(sent_len-1)*batch_size, output_size] \n",
    "        #target=[(sent_len-1)*batch_size, output_size]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        train_loss.append(epoch_loss)\n",
    "        \n",
    "    return(epoch_loss/len(iterator), train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss=0\n",
    "    val_loss=[]\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            source=batch.src\n",
    "            target=batch.trg\n",
    "            optimizer.zero_grad()\n",
    "            output=model(source, target, 0)#clip is zero and no train\n",
    "            loss=criterion(output[1:].view(-1, output.shape[2]), target[1:].view(-1))\n",
    "            epoch_loss+=loss.item()\n",
    "            val_loss.append(epoch_loss)\n",
    "        \n",
    "    return(epoch_loss/len(iterator), val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Set the path and name of the model to save them after trining them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR='models'\n",
    "model_dir=os.path.join(DIR, 'seq2seq_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test model along with analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "clip=10\n",
    "\n",
    "best_loss=float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{DIR}'):\n",
    "    os.makedirs(f'{DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number=  0\n",
      "Train loss=  4.66933508171384\n",
      "Validation loss= 4.383155465126038\n",
      "epoch number=  1\n",
      "Train loss=  3.7242734694795985\n",
      "Validation loss= 4.006883412599564\n",
      "epoch number=  2\n",
      "Train loss=  3.2578400767322155\n",
      "Validation loss= 3.7168424129486084\n",
      "epoch number=  3\n",
      "Train loss=  2.9368634633555812\n",
      "Validation loss= 3.610551595687866\n",
      "epoch number=  4\n",
      "Train loss=  2.7077340207961162\n",
      "Validation loss= 3.633746385574341\n",
      "epoch number=  5\n",
      "Train loss=  2.504183604328643\n",
      "Validation loss= 3.655905544757843\n",
      "epoch number=  6\n",
      "Train loss=  2.35830334892357\n",
      "Validation loss= 3.578988701105118\n",
      "epoch number=  7\n",
      "Train loss=  2.218757124199216\n",
      "Validation loss= 3.623843163251877\n",
      "epoch number=  8\n",
      "Train loss=  2.106057360833962\n",
      "Validation loss= 3.6249634325504303\n",
      "epoch number=  9\n",
      "Train loss=  2.0388261615442285\n",
      "Validation loss= 3.6548089385032654\n"
     ]
    }
   ],
   "source": [
    "train_losses=[]\n",
    "val_losses=[]\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_loss_list=train(model, train_it, optimizer, criterion, clip)\n",
    "    validation_loss, val_loss_list=evaluate(model, valid_it, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(validation_loss)\n",
    "    \n",
    "    if(validation_loss <best_loss):\n",
    "        best_loss=validation_loss\n",
    "        torch.save(model.state_dict(), model_dir)\n",
    "    \n",
    "    print(\"epoch number= \", epoch)\n",
    "    print(\"Train loss= \",train_loss)\n",
    "    print(\"Validation loss=\", validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if one wants to plot the train and validation loss graph \n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.plot(train_losses, val_losses)\n",
    "# plt.xlabel(\"loss\")\n",
    "# plt.ylabel(\"epochs\")\n",
    "# plt.title(\"Train vs Validation graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 3.546379894018173\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_dir))\n",
    "test_loss, test_losses=evaluate(model, test_it, criterion)\n",
    "print(\"loss=\", test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
